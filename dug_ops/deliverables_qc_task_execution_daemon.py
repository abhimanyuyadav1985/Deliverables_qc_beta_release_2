release_version = '20171216.1'

import logging
import sqlite3
import sys
import socket
import logging
import time
import os
import pickle

stream_formatter = '%(asctime)s %(levelname)-8s %(message)s'
log_level = logging.DEBUG
console_level = logging.INFO
db_file_path=os.path.join(os.getcwd(), 'task_database.sqlite3')


log_dir = os.path.join(os.getcwd(), 'Task_execution_daemon_logs')

try:
    os.stat(log_dir)
except:
    os.mkdir(log_dir)

log_name = time.strftime("%Y%m%d-%H%M%S") + '.log'
log_path = os.path.join(log_dir, log_name)
file_handler = open(log_path, 'wb')
file_handler.write('Autogenerated log file created on : ' + time.strftime('%a, %d %b %Y %H:%M:%S') + '\n')
file_handler.write('Database file : ' + db_file_path + '\n')
file_handler.write('-' * 120 + '\n')
file_handler.close()


file_handler = open(log_path, 'wb')
file_handler.write('Autogenerated log file created on : ' + time.strftime('%a, %d %b %Y %H:%M:%S') + '\n')
ip = socket.gethostbyname(socket.gethostname())
file_handler.write("System IP address : " + ip + '\n')
file_handler.write('-' * 80 + '\n')
file_handler.close()
logging.basicConfig(level=log_level,
                format='%(asctime)s %(name)-20s %(levelname)-10s %(message)s',
                filename= log_path,
                filemode='a')

logger = logging.getLogger(__name__)
console = logging.StreamHandler()
console.setLevel(console_level)
formatter = logging.Formatter(stream_formatter)
console.setFormatter(formatter)
logger.addHandler(console)

logger.info('Logging services setup done ....')
logger.info('Daemon release version: ' + release_version)


class Task_execution_service(object):
    """
    Task execution service running on the DUG tape server

    """

    def __init__(self, database_path):
        self.database_connection = False
        self.create_database_connection(database_path)

    def create_database_connection(self, database_path):
        """
        create the database connection to the specified SQLite database file
        :param database_path: path to the project SQLlite data
        :return: change the parameter self.database connection to true when connected to the database
        """
        if os.path.exists(database_path):
            logger.info('Found file now connecting to database: ' + database_path)
            try:
                self.conn = sqlite3.connect(database_path, isolation_level=None)
                self.cursor = self.conn.cursor()
                logger.info("Done.. ")
                self.database_connection = True
            except Exception as e:
                logger.error('Unable to connect to database')
                logger.error(e)
        else:
            logger.error("Unable to locate database file")

    def add_new_commands_to_database(self):
        """
        Check the fole ../from_app

        This is the folder when the pickle files for the commands from the deliverables qc application are sent
        :return: none
        """
        try:
            logger.info('Now looking for commands to add to database')
            app_command_dir = os.path.join(os.getcwd(), 'from_app')
            cmds_to_add = os.listdir(app_command_dir)
            if len(cmds_to_add) == 0:
                logger.info("No new commands to add to the database")
            else:
                for a_cmd in cmds_to_add:
                    logger.info('Found: ' + a_cmd)
                    cmd_path = os.path.join(app_command_dir, a_cmd)
                    self.add_single_cmd(cmd_path)
                logger.info("Addition of new commands to the database complete")
        except Exception as e:
            logger.error(e)

    def add_single_cmd(self, cmd_path):
        """
        Add a single command from the pickle file to the database and add to logfile
        :param cmd_path: path to the pickle file to be added to the database as a command
        :return:
        """
        try:
            file_handler = open(cmd_path, 'rb')
            cmd_tuple_to_use = pickle.load(file_handler)
            file_handler.close()
            self.cursor.execute(
                'INSERT INTO tasks(command,type,drive,sysip,submittime,logpath,status) VALUES(?,?,?,?,?,?,?)',
                cmd_tuple_to_use)
            self.conn.commit()
            logger.info('Added to database: ' + cmd_path)
            os.remove(cmd_path)
        except Exception as e:
            logger.error(e)

    def submitted_job_status_sync(self):
        """
        Sync the status for tasks with status submit, unsure, doubt and active

        submit -> unsure if missing in ps-ef or active if present

        unsure -> doubt if missing in ps -ef or active if present

        doubt -> error if missing in ps -ef or active if present

        unsure or doubt are used because some of the tape tasks do not start instantly and we may need to wait for sometime before dismissing them as errored
        :return: None
        """
        try:
            submitted_job_status_dict = {'submit': 'unsure',
                                         'unsure': 'doubt',
                                         'doubt': 'error',
                                         'active': 'finished'}
            logger.info("Now syncing submitted job status")
            self.cursor.execute("SELECT * FROM tasks WHERE status in ('submit','unsure','doubt','active')")
            self.submit_tasks = self.cursor.fetchall()
            if self.submit_tasks == None:
                logger.info("No jobs to check")
            else:
                # test comments                     
                for a_task in self.submit_tasks:
                    (id, command, type, drive, sysip, submittime, status, logpath, exe_time, finish_time,
                     exception) = a_task
                    logger.debug("-" * 80)
                    logger.debug("Test command => " + command) # This is a test string
                    cmd = str("ps -ef")
                    logger.debug("-"*80)
                    cmd_out = os.popen(cmd).readlines()
                    for a_line in cmd_out:
                        logger.debug(a_line)
                    logger.debug("-" * 80)
                    new_status = submitted_job_status_dict[status]
                    for a_line in cmd_out:
                        if command in a_line:
                            new_status = 'active'
                    if new_status == 'finished':  # finish_time needs to be added to the job when it finishes
                        finish_time = time.strftime("%Y%m%d-%H%M%S")
                        self.cursor.execute('UPDATE tasks SET status = ? , finish_time = ? WHERE id= ?',
                                            (new_status, finish_time, id))
                        self.conn.commit()
                    else:  # no need to add finish_time when the job is not finished
                        self.cursor.execute('UPDATE tasks SET status = ? WHERE id= ?', (new_status, id))
                        self.conn.commit()
                    logger.info(
                        'Status for task id: ' + str(id) + ' Changed from : ' + status + ' to: ' + new_status)
                    if type == 'segy_qc' and new_status in ['finished','error']:  # management of segy_qc_lock
                        if os.path.exists(os.path.join(os.getcwd(), 'segy_qc_lock')):
                            try:
                                os.remove(os.path.join(os.getcwd(), 'segy_qc_lock'))
                                logger.info("SEGY QC lock removed for task id: " + str(id))
                            except Exception as e:
                                logger.error(e)

        except Exception as e:
            logger.error(e)

    def submit_new_jobs(self):
        """
        Check the availability of resources and submit queued jobs to the tape server

        Execute only one SEGY QC job at a time to prevent from slowing down the system extensively

        :return: None
        """
        self.cursor.execute("SELECT * FROM tasks where status =?", ('queue',))
        self.conn.commit()
        queue_tasks = self.cursor.fetchall()
        if queue_tasks == None:
            logger.info("No tasks in queue at the moment")
        else:
            for a_task in queue_tasks:
                (id, command, type, drive, sysip, submittime, status, logpath, exe_time, finish_time, exception) = a_task
                if type == 'segy_qc':
                    if os.path.exists(os.path.join(os.getcwd(),
                                                   'segy_qc_lock')):  # prevent the execution of more than one segy qc at a time
                        logger.info("Holding Task id: " + str(id) + " another SEGY QC task is running")
                    else:
                        logger.info("Now Submitting Task id: " + str(id) + 'Command => ' + command)
                        os.system(command)
                        file_handler = open(os.path.join(os.getcwd(), 'segy_qc_lock'), 'w')
                        file_handler.close()
                        logger.info("SEGY QC lock created for execution of task id: " + str(id))
                        try:
                            self.cursor.execute("UPDATE tasks SET status = ? WHERE id=?", ('submit', id))
                            self.conn.commit()
                            logger.info('Task id: ' + str(id) + " Changed from queue to submit")
                        except exception as e:
                            logger.error(e)
                else:
                    logger.info("Now Submitting Task id: " + str(id) + 'Command => ' + command)
                    os.system(command)
                    try:
                        self.cursor.execute("UPDATE tasks SET status = ? WHERE id=?", ('submit', id))
                        self.conn.commit()
                        logger.info('Task id: ' + str(id) + " Changed from queue to submit")
                    except exception as e:
                        logger.error(e)
                exe_time = time.strftime("%Y%m%d-%H%M%S")
                self.cursor.execute('UPDATE tasks SET exe_time = ? WHERE id= ?',
                                    (exe_time, id))
                self.conn.commit()

    def create_active_task_list_for_app(self):
        """
        Extract the list of active jobs at the present moment and dump them in form of a pickled dictionary

        { drive name : task details} ::: use segy_qc as the drive for segy qc tasks

        This file is sent to application by a run information sync service using sftp and used to populate logs

        :return: None
        """
        self.app_task_sync_lock(
            mode='create')  # create the lock 1st and when this lock file exists the application will not SFTP the active tasks file
        self.cursor.execute("SELECT * FROM tasks WHERE status=?", ('active',))
        active_tasks = self.cursor.fetchall()
        active_tasks_dict = {}
        if active_tasks == None:
            logger.info("No active Tasks at the moment")
        else:
            for a_task in active_tasks:
                (
                id, command, type, drive, sysip, submittime, status, logpath, exe_time, finish_time, exception) = a_task
                key = drive
                data = a_task
                active_tasks_dict.update({key: data})
            try:
                file_handler = open(os.path.join(os.getcwd(), 'active_tasks'), 'wb')
                pickle.dump(active_tasks_dict, file_handler)
                file_handler.close()
                logger.info("Active tasks pickle file created")
            except Exception as e:
                logger.error(e)
        self.app_task_sync_lock(mode='remove')

    def app_task_sync_lock(self, mode):
        """
        Creates a file as a lock so that app does not sync the active tasks when the task execution daemaon is still writing to it

        :param mode: create ore remove the lock
        :return: none
        """
        if mode == 'create':
            try:
                file_handler = open(os.path.join(os.getcwd(), 'app_task_sync_lock'), 'w')
                file_handler.close()
                logger.info('App task sync lock created')
            except Exception as e:
                logger.error(e)
        elif mode == 'remove':
            try:
                os.remove(os.path.join(os.getcwd(), 'app_task_sync_lock'))
                logger.info('App task sync lock remvoed')
            except Exception as e:
                logger.error(e)


# -------------------------------------------------------------------------------
def main(mode):
    """
    Main function to control the execution of the deliverables_qc task execution daemon

    print the execution status to the screen

    write the detailed summary of execution and Exceptions in a log file

    wait 41 s and run again

    :param db_file_path: The full path of the database files that we need to connect with
    :param mode: 'normal' for standard execution, 'create_command' to create a test command
    :return: None

    """
    if mode == 'create_command':
        create_test_pickle()
    elif mode == 'normal':
        task_execution_service = Task_execution_service(database_path=db_file_path)
        while task_execution_service.database_connection:
            "Add all new commands supplied by the application to the database"
            logger.info("Exec => Add new commands to Database")
            task_execution_service.add_new_commands_to_database()
            logger.info("Exec => Sync Job status")
            task_execution_service.submitted_job_status_sync()
            logger.info("Exec => Create Active tasks for Application")
            task_execution_service.create_active_task_list_for_app()
            logger.info("Exec => Submit jobs from queue")
            task_execution_service.submit_new_jobs()
            logger.info("Waiting 41 sec before Next cycle")
            logger.info("+"*80)
            time.sleep(41)




# -------------------------------------------------------------------------------

def create_test_pickle():
    type = 'SEGD'
    cmd = 'I am a test command '  # This needs to be replaced by a shell command later
    drive = 'dst0'
    sysip = '10.11.1.192'
    stime = '123456'
    status = 'queue'
    log_path = 'abcdmln;kn;'
    data_to_pickle = (cmd, type, drive, sysip, stime, log_path, status,)
    cmd_pickle_path = os.path.join(os.getcwd(), 'from_app', 'test_pickle4.cmd')
    file_handler = open(cmd_pickle_path, 'wb')
    pickle.dump(data_to_pickle, file_handler)
    file_handler.close()


if __name__ == '__main__':
    main(mode='normal')  # use this option in production environment
    # main(db_file_path= sys.argv[1], mode=sys.argv[2])

